{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.13 openai==1.14.2 ragas==0.1.7 langchain-openai==0.1.1 langchain-cohere==0.1.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-0125\"\n",
    "    )\n",
    "\n",
    "embed_model=CohereEmbeddings(\n",
    "    cohere_api_key = CO_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got an [example dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval?row=1) we'll use in the next several videos in my Hugging Face repo. \n",
    "\n",
    "You don't need to sign-up for a Hugging Face account to download the repo, but if you do end up creating an acocunt [feel free to follow me](https://huggingface.co/harpreetsahota)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c2dad794a94e289be4e417f2d2dea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"explodinggradients/fiqa\", split='baseline', trust_remote_code=True)\n",
    "\n",
    "dataset = dataset.rename_column(\"ground_truths\", \"ground_truth\")\n",
    "\n",
    "# Function to concatenate list of strings into a single string\n",
    "def flatten_list_of_strings(example):\n",
    "    # Adjust 'your_list_column' to the actual column name holding the list of strings\n",
    "    example['ground_truth'] = ' '.join(example['ground_truth'])\n",
    "    return example\n",
    "\n",
    "# Apply the function to each example in the dataset\n",
    "dataset = dataset.map(flatten_list_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ” **Answer Relevancy**\n",
    "\n",
    "- ðŸŽ¯ [Answer Relevancy](https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_answer_relevance.py) measures how directly an answer addresses the question asked.\n",
    "\n",
    "- ðŸ” The process involves generating hypothetical questions from the answer and comparing these to the original question to assess similarity.\n",
    "\n",
    "- ðŸ“ Its core focus is on identifying answers that precisely address the query without veering off-topic.\n",
    "\n",
    "- ðŸ“ˆ Scoring ranges from 0 to 1, with higher scores indicating a closer match between the answer and the question.\n",
    "\n",
    "- ðŸ† The metric rewards answers that are directly applicable and penalizes those that include irrelevant details.\n",
    "\n",
    "- ðŸ“ It calculates mean cosine similarity between the original and reverse-engineered questions to quantify relevancy.\n",
    "\n",
    "$$\\text{answer relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} cos(E_{g_i}, E_o)$$\n",
    "\n",
    "\n",
    "$E_{g_i}$ is the embedding of the generated question .\n",
    "\n",
    "$E_o$ is the embedding of the original question.\n",
    "\n",
    "$N$ is the number of generated questions, which is 3 default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this work?\n",
    "\n",
    "For each provided answer, the system asks the LLM to do two things: \n",
    "\n",
    "1. Generate a question that fits the answer. \n",
    "\n",
    "2. Classify if the answer is \"noncommittal\" (evasive or not directly answering the question). \n",
    "\n",
    "The noncommittal classification is a simple 0 or 1 flag indicating if the answer directly addresses the question or dodges it. Noncommittal classification affects the relevancy score, reducing it if the answer is evasive, even if the generated question closely matches the original.\n",
    "\n",
    "This task uses the `QUESTION_GEN` prompt, effectively turning the answer (and its context) back into a question as if trying to reverse-engineer what the original question could have been. \n",
    "\n",
    "### **Question Generation and Answer Classification**\n",
    "\n",
    "  - This is a single step that accomplishes two tasks: \n",
    "    - creating a question that matches the provided answer\n",
    "    \n",
    "    - assessing the answer's directness or relevancy.\n",
    "\n",
    "   - You can specify the number of questions to generate per answer through the `strictness` argument. This attribute determines how many questions the LLM should generate for each answer, allowing for a more thorough evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import answer_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'question_generation',\n",
       " 'instruction': 'Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers',\n",
       " 'output_format_instruction': 'The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).',\n",
       " 'examples': [{'answer': 'Albert Einstein was born in Germany.',\n",
       "   'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time',\n",
       "   'output': {'question': 'Where was Albert Einstein born?',\n",
       "    'noncommittal': 0}},\n",
       "  {'answer': 'It can change its skin color based on the temperature of its environment.',\n",
       "   'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.',\n",
       "   'output': {'question': 'What unique ability does the newly discovered species of frog have?',\n",
       "    'noncommittal': 0}},\n",
       "  {'answer': 'Everest',\n",
       "   'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.',\n",
       "   'output': {'question': 'What is the tallest mountain on Earth?',\n",
       "    'noncommittal': 0}},\n",
       "  {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \",\n",
       "   'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.',\n",
       "   'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?',\n",
       "    'noncommittal': 1}}],\n",
       " 'input_keys': ['answer', 'context'],\n",
       " 'output_key': 'output',\n",
       " 'output_type': 'json',\n",
       " 'language': 'english'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_relevancy.question_generation.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing Answer Relevancy**\n",
    "\n",
    "- ðŸ“ Similarity between the original and generated questions is measured using embeddings, specifically through cosine similarity.\n",
    "\n",
    "- ðŸŽ¯ The relevancy of an answer is determined by how close the embeddings of the original question are to those of the generated question(s).\n",
    "\n",
    "- ðŸ“Š The final score averages the similarity scores across all generated questions, adjusted by the strictness setting.\n",
    "\n",
    "- ðŸ”» Noncommittal answers lead to a score penalty, ensuring only direct, relevant answers achieve high scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827b5bd463154fcd84b7d5d0a5a9bb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    llm=llm,\n",
    "    embeddings=embed_model,\n",
    "    metrics=[answer_relevancy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "**Input:** An original question and an answer (with context).\n",
    "\n",
    "**Process:** Use LLM to generate question(s) from the answer, classify committal status, calculate similarity between original and generated questions, and adjust based on committal status.\n",
    "\n",
    "**Output:** A relevancy score ranging from 0 to 1, with 1 indicating high relevancy (meaning the answer directly and accurately addresses the original question) and 0 indicating low relevancy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
