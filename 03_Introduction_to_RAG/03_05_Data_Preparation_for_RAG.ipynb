{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-readers-smart-pdf-loader pymupdf llamasherpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you will need to install the following before running this notebook:\n",
    "\n",
    "`pip install llama-index-readers-smart-pdf-loader`\n",
    "\n",
    "`pip install pymupdf`\n",
    "\n",
    "`pip install llmsherpa`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import getpass\n",
    "import nest_asyncio\n",
    "import fitz\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from text_cleaning_helpers import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Cleaning for RAG\n",
    "\n",
    "Your RAG system is only as good as the data you retrieve. \n",
    "\n",
    "That's why data preparation and cleaning are important steps to ensure high-quality results. **This course purposefully uses simple PDF files, specifically books, to demonstrate the process.** There's so much to data preparation for RAG that I could write another two-hour course just on that topic. However, it's important to acknowledge that real-world PDFs and other documents can be much more complex, requiring additional processing and cleaning techniques.\n",
    "\n",
    "### Considerations for data prep\n",
    "\n",
    "- üìú **Document Content**: Utilize text from documents for keyword searches or to find similar content in RAG applications.\n",
    "\n",
    "- üìë **Document Elements**: Break down documents into fundamental parts to assist in RAG tasks like filtering and segmenting, like:\n",
    "  - Titles\n",
    "  - Narrative text\n",
    "  - List items\n",
    "  - Tables\n",
    "  - Images\n",
    "\n",
    "- üè∑ **Element Metadata**: Provide additional details for each document element to support hybrid search and track information origin, such as:\n",
    "  - Filename\n",
    "  - Filetype\n",
    "  - Page number\n",
    "  - Section\n",
    "\n",
    "- üîÑ **Summary**: Explains document preprocessing for retrieval systems, focusing on transforming documents into searchable elements and metadata.\n",
    "\n",
    "\n",
    "\n",
    "#### Let's inspect our PDFs\n",
    "\n",
    "**Now that the disclaimer is out of the way, let's work with the PDFs that we have.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"../data/almanack_of_naval_ravikant.pdf\"\n",
    "\n",
    "LLMSHERPA_API_URL = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.readers.smart_pdf_loader import SmartPDFLoader\n",
    "\n",
    "simple_directory_reader_docs = SimpleDirectoryReader(input_files=[PDF_PATH]).load_data()\n",
    "\n",
    "smart_pdf_loader_docs = SmartPDFLoader(llmsherpa_api_url=LLMSHERPA_API_URL).load_data(PDF_PATH)\n",
    "\n",
    "pdf_reader_docs = PDFReader().load_data(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simple_directory_reader_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_directory_reader_docs[100].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(smart_pdf_loader_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smart_pdf_loader_docs[100].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_reader_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_reader_docs[100].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader_docs[100].text == simple_directory_reader_docs[100].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = fitz.open(PDF_PATH)\n",
    "\n",
    "def extract_text(document, opt=\"text\"):\n",
    "    '''Extract text from a page and returns a list of strings'''\n",
    "    text = document.get_text(opt, sort=True) \n",
    "    text = text.split(\"\\n\")\n",
    "    return text\n",
    "\n",
    "pages = [extract_text(page) for page in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[42] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(file_path, pages):\n",
    "    \"\"\"\n",
    "    Opens a PDF file and optionally selects specific pages to create a document object.\n",
    "\n",
    "    This function utilizes the `fitz` library to open a PDF file located at `file_path`. \n",
    "    If a list of `pages` is provided, the function selects only these pages from the document.\n",
    "    This is useful for focusing on certain parts of a PDF without loading the entire document into memory.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the PDF file to be opened.\n",
    "        pages (list of int, optional): A list of page numbers to select from the PDF. \n",
    "            If `None`, the entire document is loaded.\n",
    "\n",
    "    \"\"\"\n",
    "    document = fitz.open(file_path)\n",
    "    if pages is not None:\n",
    "        document.select(pages)  # Select specific pages if pages are provided\n",
    "    return document\n",
    "\n",
    "\n",
    "def handle_chapter_headers_footers(strings, flag):\n",
    "    \"\"\"\n",
    "    Modify a list of strings based on a specified flag and join them into a single string.\n",
    "\n",
    "    This function first removes any empty strings from the input list. It then checks if the\n",
    "    remaining list has more than three elements. If so, it modifies the list by removing the\n",
    "    first element, last element, or both, based on the value of the flag. The final list is then\n",
    "    joined into a single string with spaces separating the elements.\n",
    "\n",
    "    Parameters:\n",
    "        strings (list of str): The list of strings to modify.\n",
    "        flag (str): A flag indicating the modification to perform on the list:\n",
    "            - 'remove_first': Remove the first element of the list.\n",
    "            - 'remove_last': Remove the last element of the list.\n",
    "            - 'remove_first_last': Remove both the first and last elements of the list.\n",
    "            - 'remove_first_two': Remove the first two elements of the list.\n",
    "            - Any other value leaves the list unchanged.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string composed of the modified list elements, separated by spaces.\n",
    "    \"\"\"\n",
    "    # Filter out empty strings\n",
    "    filtered_strings = [s for s in strings if s]\n",
    "    \n",
    "    # Check if the filtered list has more than three elements\n",
    "    if len(filtered_strings) > 3:\n",
    "        if flag == 'remove_first':\n",
    "            filtered_strings = filtered_strings[1:]  # Slice off the first element\n",
    "        elif flag == 'remove_last':\n",
    "            filtered_strings = filtered_strings[:-1]  # Slice off the last element\n",
    "        elif flag == 'remove_first_last':\n",
    "            filtered_strings = filtered_strings[1:-1]  # Slice off the first and last elements\n",
    "        elif flag == 'remove_first_two':\n",
    "            filtered_strings = filtered_strings[2:]  # Slice off the first two elements\n",
    "    \n",
    "    # Join all strings with a space and return the result\n",
    "    return ' '.join(filtered_strings).strip()\n",
    "\n",
    "def extract_text(page, file_name, title, author, flag, opt=\"text\"):\n",
    "    \"\"\"\n",
    "    Extracts text from a specified page of a document and returns a dictionary containing\n",
    "    the extracted text and associated metadata.\n",
    "\n",
    "    The function first retrieves text from the given `page` object using the specified `opt` method.\n",
    "    It then processes this text to remove chapter headers, footers, and applies various cleaning\n",
    "    procedures according to the `flag` and other parameters set in the `clean` function.\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The page object from which to extract text.\n",
    "        file_name (str): The name of the file from which the page is taken.\n",
    "        title (str): The title of the document.\n",
    "        author (str): The author of the document.\n",
    "        flag (str): A flag used to customize how chapter headers and footers are handled.\n",
    "        opt (str, optional): The method of text extraction to be used by `get_text`.\n",
    "            Defaults to \"text\", but can be changed to other methods supported by the library.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with two keys:\n",
    "            - 'text': A string containing the cleaned and processed text from the page.\n",
    "            - 'metadata': A dictionary containing metadata about the text, including the\n",
    "                          page number, file name, title, and author.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = page.get_text(opt, sort=True)\n",
    "\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "    text = handle_chapter_headers_footers(text, flag)\n",
    "\n",
    "    text = clean(\n",
    "        text,\n",
    "        extra_whitespace=True,\n",
    "        broken_paragraphs=True,\n",
    "        bullets=True,\n",
    "        ascii=True,\n",
    "        lowercase=False,\n",
    "        citations=True,\n",
    "        merge_split_words=True,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"page_number\": page.number,\n",
    "            \"file_name\": file_name,\n",
    "            \"title\": title,\n",
    "            \"author\": author\n",
    "        }\n",
    "    }\n",
    "\n",
    "def extract_texts_from_pdf(file_path, title, author, pages, flag):\n",
    "    document = get_document(file_path, pages)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    extracted_texts = [extract_text(page, file_path, title, author, flag) for page in document]\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\n",
    "    {\n",
    "        \"file_path\": \"../data/almanack_of_naval_ravikant.pdf\", \n",
    "        \"title\": \"The Almanack of Naval Ravikant\", \n",
    "        \"author\": \"Naval Ravikant\", \n",
    "        \"pages\": list(range(29, 203)),\n",
    "        \"flag\": \"remove_last\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/anthology_of_balaji.pdf\", \n",
    "        \"title\": \"The Anthology of Balaji Srinivasan\", \n",
    "        \"author\": \"Balaji Srinivasan\", \n",
    "        \"pages\": list(range(32, 261)),\n",
    "        \"flag\": \"remove_last\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/hackers_and_painters.pdf\", \n",
    "        \"title\": \"Hackers and Painters\", \n",
    "        \"author\": \"Paul Graham\", \n",
    "        \"pages\": list(range(14,221)),\n",
    "        \"flag\": \"remove_first_last\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/skin_in_the_game.pdf\", \n",
    "        \"title\": \"Skin in the Game\", \n",
    "        \"author\": \"Nassim Nicholas Taleb\", \n",
    "        \"pages\": list(range(15,272)),\n",
    "        \"flag\": None\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/taoofseneca_vol1-1.pdf\", \n",
    "        \"title\": \"Letters From a Stoic Volume 1\",\n",
    "        \"author\": \"Seneca\", \n",
    "        \"pages\": list(range(15,308)),\n",
    "        \"flag\": \"remove_first_two\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/taoofseneca_vol2.pdf\", \n",
    "        \"title\": \"Letters From a Stoic Volume 2\",  \n",
    "        \"author\": \"Seneca\", \n",
    "        \"pages\": list(range(7,283)),\n",
    "        \"flag\": \"remove_first_two\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/taoofseneca_vol3.pdf\", \n",
    "        \"title\": \"Letters From a Stoic Volume 3\",  \n",
    "        \"author\": \"Seneca\", \n",
    "        \"pages\": list(range(7,258)),\n",
    "        \"flag\": \"remove_first_two\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/striking-thoughts.pdf\", \n",
    "        \"title\": \"Striking Thoughts\",  \n",
    "        \"author\": \"Bruce Lee\", \n",
    "        \"pages\": list(range(20,217)),\n",
    "        \"flag\": None\n",
    "        },\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    print(f\"Extracting texts from {pdf['title']} by {pdf['author']}...\")\n",
    "    texts = extract_texts_from_pdf(pdf[\"file_path\"], pdf[\"title\"], pdf[\"author\"], pdf[\"pages\"], pdf[\"flag\"])\n",
    "    print(f\"Finished extracting texts from {pdf['title']}.\")\n",
    "    all_texts.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and persist a Document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "llama_index_docs = [Document(text=doc[\"text\"], metadata=doc[\"metadata\"]) for doc in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llama_index_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_index_docs[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage import StorageContext\n",
    "\n",
    "# Create a SimpleDocumentStore and add the documents\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(llama_index_docs)\n",
    "\n",
    "# Create a storage context\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "# Persist the document store to disk\n",
    "storage_context.persist(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges with Complex PDFs and Documents\n",
    "\n",
    "1. üìë **Formatting inconsistencies**: PDFs and other documents can have varying layouts, fonts, and styles, making it difficult to extract text consistently.\n",
    "\n",
    "2. üèûÔ∏è **Images and graphics**: Documents may contain images, charts, and other visual elements that need to be handled separately or extracted using Optical Character Recognition (OCR) techniques.\n",
    "\n",
    "3. üíΩ **Tables and structured data**: Extracting information from tables and structured data within documents can be challenging and may require specialized tools or techniques.\n",
    "\n",
    "4. üíæ **Metadata and noise**: Documents may include metadata, headers, footers, and other noise that needs to be handled before processing.\n",
    "\n",
    "While this course won't cover these complex scenarios in depth, it's essential to understand the potential challenges and the need for more advanced data preparation and cleaning techniques when working with diverse document types.\n",
    "\n",
    "## Options for parsing complex pdfs\n",
    "\n",
    "### General PDFs\n",
    "\n",
    " - [LlamaParse](https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/llama_parse/) - LlamaParse is an API created by LlamaIndex to efficiently parse and represent files for efficient retrieval and context augmentation using LlamaIndex frameworks.\n",
    "\n",
    " - [pdfminer.six](https://pdfminersix.readthedocs.io/en/latest/) - A tool for extracting information from PDF documents. It focuses on getting and analyzing text data.\n",
    "\n",
    "- [pdfplumber](https://github.com/jsvine/pdfplumber) - Gives you detailed information about each text character, rectangle, and line. Plus: Table extraction and visual debugging.\n",
    "\n",
    "- [pypdf](https://pypdf.readthedocs.io/en/latest/) - Capable of splitting, merging, cropping, and transforming the pages of PDF files.\n",
    "\n",
    "- [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/) - A high-performance Python library for data extraction, analysis, conversion & manipulation of PDF (and other) documents.\n",
    "\n",
    "- [Camelot](https://camelot-py.readthedocs.io/en/master/) - This library is specifically for extracting data from tables in PDFs. This repo also has a [nice comparison](https://github.com/camelot-dev/camelot/wiki/Comparison-with-other-PDF-Table-Extraction-libraries-and-tools) of other table extraction libraries.\n",
    "\n",
    "- [LLMSherpa](https://github.com/nlmatics/llmsherpa) - The main class here is the `LayoutPDFReader`, and a good read about the problem and proposed solution is [here](https://ambikasukla.substack.com/p/efficient-rag-with-document-layout)\n",
    "\n",
    "- [unstructured](https://github.com/Unstructured-IO/unstructured) - This has components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more. \n",
    "\n",
    "- [Table Transformer](https://github.com/microsoft/table-transformer) - A deep learning model for extracting tables from unstructured documents (PDFs and images)\n",
    "\n",
    "- [Layout Parser](https://github.com/Layout-Parser/layout-parser) - This is a unified toolkit for deep learning based document image analysis which has a rich repository of deep learning models for layout detection, as well as a set of unified APIs for using them.\n",
    "\n",
    " - [marker](https://github.com/VikParuchuri/marker) - Converts PDF to markdown quickly with high accuracy.\n",
    "\n",
    " - [surya](https://github.com/VikParuchuri/surya) -  A document OCR toolkit for accurate OCR in 90+ languages, line-level text detection in any language, layout analysis (table, image, header, etc detection) in any language.\n",
    "\n",
    "### Academic PDFs\n",
    "\n",
    "- [nougat](https://github.com/facebookresearch/nougat) - This is an academic document PDF parser that understands LaTeX math and tables.\n",
    "\n",
    "- [GROBID](https://grobid.readthedocs.io/en/latest/Introduction/) - This is a a machine learning library for extracting, parsing and re-structuring raw documents such as PDF into structured XML/TEI encoded documents with a particular focus on technical and scientific publications.\n",
    "\n",
    "- [LaTeX-OCR](https://github.com/lukas-blecher/LaTeX-OCR/) - Uses a vision transformer (ViT) to convert images of equations into LaTeX code.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
