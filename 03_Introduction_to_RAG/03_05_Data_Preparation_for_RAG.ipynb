{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-readers-smart-pdf-loader pymupdf llamasherpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you will need to install the following before running this notebook:\n",
    "\n",
    "`pip install llama-index-readers-smart-pdf-loader`\n",
    "\n",
    "`pip install pymupdf`\n",
    "\n",
    "`pip install llmsherpa`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import getpass\n",
    "import nest_asyncio\n",
    "import fitz\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from text_cleaning_helpers import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Cleaning for RAG\n",
    "\n",
    "Your RAG system is only as good as the data you retrieve. \n",
    "\n",
    "That's why data preparation and cleaning are important steps to ensure high-quality results. **This course purposefully uses simple PDF files, specifically books, to demonstrate the process.** There's so much to data preparation for RAG that I could write another two-hour course just on that topic. However, it's important to acknowledge that real-world PDFs and other documents can be much more complex, requiring additional processing and cleaning techniques.\n",
    "\n",
    "### Considerations for data prep\n",
    "\n",
    "- üìú **Document Content**: Utilize text from documents for keyword searches or to find similar content in RAG applications.\n",
    "\n",
    "- üìë **Document Elements**: Break down documents into fundamental parts to assist in RAG tasks like filtering and segmenting, like:\n",
    "  - Titles\n",
    "  - Narrative text\n",
    "  - List items\n",
    "  - Tables\n",
    "  - Images\n",
    "\n",
    "- üè∑ **Element Metadata**: Provide additional details for each document element to support hybrid search and track information origin, such as:\n",
    "  - Filename\n",
    "  - Filetype\n",
    "  - Page number\n",
    "  - Section\n",
    "\n",
    "- üîÑ **Summary**: Explains document preprocessing for retrieval systems, focusing on transforming documents into searchable elements and metadata.\n",
    "\n",
    "\n",
    "\n",
    "#### Let's inspect our PDFs\n",
    "\n",
    "**Now that the disclaimer is out of the way, let's work with the PDFs that we have.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"../data/almanack_of_naval_ravikant.pdf\"\n",
    "\n",
    "LLMSHERPA_API_URL = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.readers.smart_pdf_loader import SmartPDFLoader\n",
    "\n",
    "simple_directory_reader_docs = SimpleDirectoryReader(input_files=[PDF_PATH]).load_data()\n",
    "\n",
    "smart_pdf_loader_docs = SmartPDFLoader(llmsherpa_api_url=LLMSHERPA_API_URL).load_data(PDF_PATH)\n",
    "\n",
    "pdf_reader_docs = PDFReader().load_data(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple_directory_reader_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING  JUDGMENT ¬∑  101SHED YOUR IDENTITY TO SEE REALITY\n",
      "Our egos are constructed in our formative years‚Äîour first \n",
      "two decades. They get constructed by our environment, our \n",
      "parents, society. Then, we spend the rest of our life trying to \n",
      "make our ego happy. We interpret anything new through our \n",
      "ego: ‚ÄúHow do I change the external world to make it more how \n",
      "I would like it to be?‚Äù¬†[8]\n",
      "‚ÄúTension is who you think you should be.  \n",
      "Relaxation is who you are.‚Äù\n",
      "‚ÄîBuddhist saying\n",
      "You absolutely need habits to function. You cannot solve every \n",
      "problem in life as if it is the first time it‚Äôs thrown at you. We \n",
      "accumulate all these habits. We put them in the bundle of \n",
      "identity, ego, ourselves, and then we get attached to them. ‚ÄúI‚Äôm \n",
      "Naval. This is the way I am.‚Äù\n",
      "It‚Äô s really important to be able to uncondition yourself, to be \n",
      "able to take your habits apart and say, ‚ÄúOkay, this is a habit I \n",
      "probably picked up when I was a toddler trying to get my par-\n",
      "ent‚Äôs attention. Now I‚Äôve reinforced it and reinforced it, and \n",
      "I call it a part of my identity. Does it still serve me? Does it \n",
      "make me happier? Does it make me healthier? Does it make \n",
      "me accomplish whatever I set out to accomplish?‚Äù\n",
      "I‚Äôm less habitual than most people. I don‚Äôt like to structure \n",
      "my day. To the extent I have habits, I try to make them more \n",
      "deliberate rather than accidents of history.¬†[4]\n"
     ]
    }
   ],
   "source": [
    "print(simple_directory_reader_docs[100].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smart_pdf_loader_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART I WEALTH > BUILDING WEALTH > UNDERSTAND HOW WEALTH IS CREATED > How to Get Rich (Without Getting Lucky):\n",
      "‚Üì The internet has massively broadened the possible space of careers.\n",
      "Most people haven‚Äôt figured this out yet.\n"
     ]
    }
   ],
   "source": [
    "print(smart_pdf_loader_docs[100].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_reader_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING  JUDGMENT ¬∑  101SHED YOUR IDENTITY TO SEE REALITY\n",
      "Our egos are constructed in our formative years‚Äîour first \n",
      "two decades. They get constructed by our environment, our \n",
      "parents, society. Then, we spend the rest of our life trying to \n",
      "make our ego happy. We interpret anything new through our \n",
      "ego: ‚ÄúHow do I change the external world to make it more how \n",
      "I would like it to be?‚Äù¬†[8]\n",
      "‚ÄúTension is who you think you should be.  \n",
      "Relaxation is who you are.‚Äù\n",
      "‚ÄîBuddhist saying\n",
      "You absolutely need habits to function. You cannot solve every \n",
      "problem in life as if it is the first time it‚Äôs thrown at you. We \n",
      "accumulate all these habits. We put them in the bundle of \n",
      "identity, ego, ourselves, and then we get attached to them. ‚ÄúI‚Äôm \n",
      "Naval. This is the way I am.‚Äù\n",
      "It‚Äô s really important to be able to uncondition yourself, to be \n",
      "able to take your habits apart and say, ‚ÄúOkay, this is a habit I \n",
      "probably picked up when I was a toddler trying to get my par-\n",
      "ent‚Äôs attention. Now I‚Äôve reinforced it and reinforced it, and \n",
      "I call it a part of my identity. Does it still serve me? Does it \n",
      "make me happier? Does it make me healthier? Does it make \n",
      "me accomplish whatever I set out to accomplish?‚Äù\n",
      "I‚Äôm less habitual than most people. I don‚Äôt like to structure \n",
      "my day. To the extent I have habits, I try to make them more \n",
      "deliberate rather than accidents of history.¬†[4]\n"
     ]
    }
   ],
   "source": [
    "print(pdf_reader_docs[100].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader_docs[100].text == simple_directory_reader_docs[100].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = fitz.open(PDF_PATH)\n",
    "\n",
    "def extract_text(document, opt=\"text\"):\n",
    "    '''Extract text from a page and returns a list of strings'''\n",
    "    text = document.get_text(opt, sort=True) \n",
    "    text = text.split(\"\\n\")\n",
    "    return text\n",
    "\n",
    "pages = [extract_text(page) for page in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have some sales skills, which is a form of specific knowledge. ',\n",
       " 'I have some analytical skills on how to make money. And I ',\n",
       " 'have this ability to absorb data, obsess about it, and break it ',\n",
       " 'down‚Äîthat is a specific skill that I have. I also love tinkering ',\n",
       " 'with technology. And all of this stuff feels like play to me, but ',\n",
       " 'it looks like work to others.',\n",
       " 'There are other people to whom these things would be hard, ',\n",
       " 'and they say, ‚ÄúWell, how do I get good at being pithy and sell-',\n",
       " 'ing ideas?‚Äù Well, if you‚Äôre not already good at it or if you‚Äôre not ',\n",
       " 'really into it, maybe it‚Äôs not your thing‚Äîfocus on the thing ',\n",
       " 'that you are really into.',\n",
       " 'The first person to actually point out my real specific knowl-',\n",
       " 'edge was my mother. She did it as an aside, talking from the ',\n",
       " 'kitchen, and she said it when I was fifteen or sixteen years old. ',\n",
       " 'I was telling a friend of mine that I want to be an astrophysi-',\n",
       " 'cist, and she said, ‚ÄúNo, you‚Äôre going to go into business.‚Äù I was ',\n",
       " 'like, ‚ÄúWhat, my mom‚Äôs telling me I‚Äôm going to be in business? ',\n",
       " 'I‚Äôm going to be an astrophysicist. Mom doesn‚Äôt know she‚Äôs ',\n",
       " 'talking about.‚Äù But Mom knew exactly what she was talking ',\n",
       " 'about.\\xa0[78]',\n",
       " 'Specific knowledge is found much more by pursuing your ',\n",
       " 'innate talents, your genuine curiosity, and your passion. It‚Äôs ',\n",
       " 'not by going to school for whatever is the hottest job; it‚Äôs not ',\n",
       " 'by going into whatever field investors say is the hottest.',\n",
       " 'Very often, specific knowledge is at the edge of knowledge. It‚Äôs ',\n",
       " 'also stuff that‚Äôs only now being figured out or is really hard to ',\n",
       " 'figure out. If you‚Äôre not 100 percent into it, somebody else who ',\n",
       " 'is 100 percent into it will outperform you. And they won‚Äôt just ',\n",
       " 'outperform you by a little bit‚Äîthey‚Äôll outperform you by a lot ',\n",
       " 'B u i l d i n g  W e a l t h \\u2002 ¬∑ \\u2002 43',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[42] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(file_path, pages):\n",
    "    \"\"\"\n",
    "    Opens a PDF file and optionally selects specific pages to create a document object.\n",
    "\n",
    "    This function utilizes the `fitz` library to open a PDF file located at `file_path`. \n",
    "    If a list of `pages` is provided, the function selects only these pages from the document.\n",
    "    This is useful for focusing on certain parts of a PDF without loading the entire document into memory.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the PDF file to be opened.\n",
    "        pages (list of int, optional): A list of page numbers to select from the PDF. \n",
    "            If `None`, the entire document is loaded.\n",
    "\n",
    "    \"\"\"\n",
    "    document = fitz.open(file_path)\n",
    "    if pages is not None:\n",
    "        document.select(pages)  # Select specific pages if pages are provided\n",
    "    return document\n",
    "\n",
    "\n",
    "def handle_chapter_headers_footers(strings, flag):\n",
    "    \"\"\"\n",
    "    Modify a list of strings based on a specified flag and join them into a single string.\n",
    "\n",
    "    This function first removes any empty strings from the input list. It then checks if the\n",
    "    remaining list has more than three elements. If so, it modifies the list by removing the\n",
    "    first element, last element, or both, based on the value of the flag. The final list is then\n",
    "    joined into a single string with spaces separating the elements.\n",
    "\n",
    "    Parameters:\n",
    "        strings (list of str): The list of strings to modify.\n",
    "        flag (str): A flag indicating the modification to perform on the list:\n",
    "            - 'remove_first': Remove the first element of the list.\n",
    "            - 'remove_last': Remove the last element of the list.\n",
    "            - 'remove_first_last': Remove both the first and last elements of the list.\n",
    "            - 'remove_first_two': Remove the first two elements of the list.\n",
    "            - Any other value leaves the list unchanged.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string composed of the modified list elements, separated by spaces.\n",
    "    \"\"\"\n",
    "    # Filter out empty strings\n",
    "    filtered_strings = [s for s in strings if s]\n",
    "    \n",
    "    # Check if the filtered list has more than three elements\n",
    "    if len(filtered_strings) > 3:\n",
    "        if flag == 'remove_first':\n",
    "            filtered_strings = filtered_strings[1:]  # Slice off the first element\n",
    "        elif flag == 'remove_last':\n",
    "            filtered_strings = filtered_strings[:-1]  # Slice off the last element\n",
    "        elif flag == 'remove_first_last':\n",
    "            filtered_strings = filtered_strings[1:-1]  # Slice off the first and last elements\n",
    "        elif flag == 'remove_first_two':\n",
    "            filtered_strings = filtered_strings[2:]  # Slice off the first two elements\n",
    "    \n",
    "    # Join all strings with a space and return the result\n",
    "    return ' '.join(filtered_strings).strip()\n",
    "\n",
    "def extract_text(page, file_name, title, author, flag, opt=\"text\"):\n",
    "    \"\"\"\n",
    "    Extracts text from a specified page of a document and returns a dictionary containing\n",
    "    the extracted text and associated metadata.\n",
    "\n",
    "    The function first retrieves text from the given `page` object using the specified `opt` method.\n",
    "    It then processes this text to remove chapter headers, footers, and applies various cleaning\n",
    "    procedures according to the `flag` and other parameters set in the `clean` function.\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The page object from which to extract text.\n",
    "        file_name (str): The name of the file from which the page is taken.\n",
    "        title (str): The title of the document.\n",
    "        author (str): The author of the document.\n",
    "        flag (str): A flag used to customize how chapter headers and footers are handled.\n",
    "        opt (str, optional): The method of text extraction to be used by `get_text`.\n",
    "            Defaults to \"text\", but can be changed to other methods supported by the library.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with two keys:\n",
    "            - 'text': A string containing the cleaned and processed text from the page.\n",
    "            - 'metadata': A dictionary containing metadata about the text, including the\n",
    "                          page number, file name, title, and author.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = page.get_text(opt, sort=True)\n",
    "\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "    text = handle_chapter_headers_footers(text, flag)\n",
    "\n",
    "    text = clean(\n",
    "        text,\n",
    "        extra_whitespace=True,\n",
    "        broken_paragraphs=True,\n",
    "        bullets=True,\n",
    "        ascii=True,\n",
    "        lowercase=False,\n",
    "        citations=True,\n",
    "        merge_split_words=True,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"page_number\": page.number,\n",
    "            \"file_name\": file_name,\n",
    "            \"title\": title,\n",
    "            \"author\": author\n",
    "        }\n",
    "    }\n",
    "\n",
    "def extract_texts_from_pdf(file_path, title, author, pages, flag):\n",
    "    document = get_document(file_path, pages)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    extracted_texts = [extract_text(page, file_path, title, author, flag) for page in document]\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting texts from The Almanack of Naval Ravikant by Naval Ravikant...\n",
      "Finished extracting texts from The Almanack of Naval Ravikant.\n",
      "Extracting texts from The Anthology of Balaji Srinivasan by Balaji Srinivasan...\n",
      "Finished extracting texts from The Anthology of Balaji Srinivasan.\n",
      "Extracting texts from Hackers and Painters by Paul Graham...\n",
      "Finished extracting texts from Hackers and Painters.\n",
      "Extracting texts from Skin in the Game by Nassim Nicholas Taleb...\n",
      "Finished extracting texts from Skin in the Game.\n",
      "Extracting texts from Letters From a Stoic Volume 1 by Seneca...\n",
      "Finished extracting texts from Letters From a Stoic Volume 1.\n",
      "Extracting texts from Letters From a Stoic Volume 2 by Seneca...\n",
      "Finished extracting texts from Letters From a Stoic Volume 2.\n",
      "Extracting texts from Letters From a Stoic Volume 3 by Seneca...\n",
      "Finished extracting texts from Letters From a Stoic Volume 3.\n",
      "Extracting texts from Striking Thoughts by Bruce Lee...\n",
      "Finished extracting texts from Striking Thoughts.\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\n",
    "    {\n",
    "        \"file_path\": \"../data/almanack_of_naval_ravikant.pdf\", \n",
    "        \"title\": \"The Almanack of Naval Ravikant\", \n",
    "        \"author\": \"Naval Ravikant\", \n",
    "        \"pages\": list(range(29, 203)),\n",
    "        \"flag\": \"remove_last\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/anthology_of_balaji.pdf\", \n",
    "        \"title\": \"The Anthology of Balaji Srinivasan\", \n",
    "        \"author\": \"Balaji Srinivasan\", \n",
    "        \"pages\": list(range(32, 261)),\n",
    "        \"flag\": \"remove_last\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/hackers_and_painters.pdf\", \n",
    "        \"title\": \"Hackers and Painters\", \n",
    "        \"author\": \"Paul Graham\", \n",
    "        \"pages\": list(range(14,221)),\n",
    "        \"flag\": \"remove_first_last\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/skin_in_the_game.pdf\", \n",
    "        \"title\": \"Skin in the Game\", \n",
    "        \"author\": \"Nassim Nicholas Taleb\", \n",
    "        \"pages\": list(range(15,272)),\n",
    "        \"flag\": None\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/taoofseneca_vol1-1.pdf\", \n",
    "        \"title\": \"Letters From a Stoic Volume 1\",\n",
    "        \"author\": \"Seneca\", \n",
    "        \"pages\": list(range(15,308)),\n",
    "        \"flag\": \"remove_first_two\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/taoofseneca_vol2.pdf\", \n",
    "        \"title\": \"Letters From a Stoic Volume 2\",  \n",
    "        \"author\": \"Seneca\", \n",
    "        \"pages\": list(range(7,283)),\n",
    "        \"flag\": \"remove_first_two\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/taoofseneca_vol3.pdf\", \n",
    "        \"title\": \"Letters From a Stoic Volume 3\",  \n",
    "        \"author\": \"Seneca\", \n",
    "        \"pages\": list(range(7,258)),\n",
    "        \"flag\": \"remove_first_two\"\n",
    "        },\n",
    "    {\n",
    "        \"file_path\": \"../data/striking-thoughts.pdf\", \n",
    "        \"title\": \"Striking Thoughts\",  \n",
    "        \"author\": \"Bruce Lee\", \n",
    "        \"pages\": list(range(20,217)),\n",
    "        \"flag\": None\n",
    "        },\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    print(f\"Extracting texts from {pdf['title']} by {pdf['author']}...\")\n",
    "    texts = extract_texts_from_pdf(pdf[\"file_path\"], pdf[\"title\"], pdf[\"author\"], pdf[\"pages\"], pdf[\"flag\"])\n",
    "    print(f\"Finished extracting texts from {pdf['title']}.\")\n",
    "    all_texts.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1884"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Set a very high hourly aspirational rate for yourself and stick to it. It should seem and feel absurdly high. If it doesnt, its not high enough. Whatever you picked, my advice to you would be to raise it. Like I said, for myself, even before I had money, for the longest time I used $5,000 an hour. And if you extrapolate that out into what it looks like as an annual salary, its multiple millions of dollars per year. Ironically, I actually think Ive beaten it. Im not the hardest working personIm actually a lazy person. I work through bursts of energy where Im really motivated with something. If I actually look at how much Ive earned per actual hour that Ive put in, its probably quite a bit higher than that. Can you expand on your statement, If you secretly despise wealth, it will elude you? If you get into a relative mindset, youre always going to hate people who do better than you, youre always going to be jealous or envious of them. Theyll sense those feelings when you try and do business with them. When you try and do business with somebody, if you have any bad thoughts or any judgments about them, they will feel it. Humans are wired to feel what the other person deep down inside feels. You have to get out of a relative mindset. Literally, being anti-wealth will prevent you from becoming wealthy, because you will not have the right mindset for it, you wont have the right spirit, and you wont be dealing with people on the right level. Be optimistic, be positive. Its important. Optimists actually do better in the long run.',\n",
       " 'metadata': {'page_number': 42,\n",
       "  'file_name': '../data/almanack_of_naval_ravikant.pdf',\n",
       "  'title': 'The Almanack of Naval Ravikant',\n",
       "  'author': 'Naval Ravikant'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and persist a Document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "llama_index_docs = [Document(text=doc[\"text\"], metadata=doc[\"metadata\"]) for doc in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1884"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llama_index_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '36194420-275a-4959-923d-bac8023b69f9',\n",
       " 'embedding': None,\n",
       " 'metadata': {'page_number': 0,\n",
       "  'file_name': '../data/almanack_of_naval_ravikant.pdf',\n",
       "  'title': 'The Almanack of Naval Ravikant',\n",
       "  'author': 'Naval Ravikant'},\n",
       " 'excluded_embed_metadata_keys': [],\n",
       " 'excluded_llm_metadata_keys': [],\n",
       " 'relationships': {},\n",
       " 'text': 'UNDERSTAND HOW WEALTH IS CREATED I like to think that if I lost all my money and you dropped me on a random street in any English-speaking country, within five or ten years Id be wealthy again because its just a skillset Ive developed that anyone can develop. Its not really about hard work. You can work in a restaurant eighty hours a week, and youre not going to get rich. Getting rich is about knowing what to do, who to do it with, and when to do it. It is much more about understanding than purely hard work. Yes, hard work matters, and you cant skimp on it. But it has to be directed in the right way. If you dont know yet what you should work on, the most important thing is to figure it out. You should not grind at a lot of hard work until you figure out what you should be working on. I came up with the principles in my tweetstorm (below) for myself when I was really young, around thirteen or fourteen. Ive been carrying them in my head for thirty years, and Ive been living them. Over time (sadly or fortunately), the thing I got really good at was looking at businesses and figuring out the point of maximum leverage to actually create wealth and capture some of that created wealth. This is exactly what I did my famous tweetstorm about. Of course, every one of these tweets can be extrapolated into an hours worth of conversation. The tweetstorm below is a good starting point. The tweetstorm tries to be information-dense, very concise, high-impact, and timeless. It has all the information and principles, so if you absorb these and you work hard over ten years, youll get what you want.',\n",
       " 'start_char_idx': None,\n",
       " 'end_char_idx': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_index_docs[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage import StorageContext\n",
    "\n",
    "# Create a SimpleDocumentStore and add the documents\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(llama_index_docs)\n",
    "\n",
    "# Create a storage context\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "# Persist the document store to disk\n",
    "storage_context.persist(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges with Complex PDFs and Documents\n",
    "\n",
    "1. üìë **Formatting inconsistencies**: PDFs and other documents can have varying layouts, fonts, and styles, making it difficult to extract text consistently.\n",
    "\n",
    "2. üèûÔ∏è **Images and graphics**: Documents may contain images, charts, and other visual elements that need to be handled separately or extracted using Optical Character Recognition (OCR) techniques.\n",
    "\n",
    "3. üíΩ **Tables and structured data**: Extracting information from tables and structured data within documents can be challenging and may require specialized tools or techniques.\n",
    "\n",
    "4. üíæ **Metadata and noise**: Documents may include metadata, headers, footers, and other noise that needs to be handled before processing.\n",
    "\n",
    "While this course won't cover these complex scenarios in depth, it's essential to understand the potential challenges and the need for more advanced data preparation and cleaning techniques when working with diverse document types.\n",
    "\n",
    "## Options for parsing complex pdfs\n",
    "\n",
    "### General PDFs\n",
    "\n",
    " - [LlamaParse](https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/llama_parse/) - LlamaParse is an API created by LlamaIndex to efficiently parse and represent files for efficient retrieval and context augmentation using LlamaIndex frameworks.\n",
    "\n",
    " - [pdfminer.six](https://pdfminersix.readthedocs.io/en/latest/) - A tool for extracting information from PDF documents. It focuses on getting and analyzing text data.\n",
    "\n",
    "- [pdfplumber](https://github.com/jsvine/pdfplumber) - Gives you detailed information about each text character, rectangle, and line. Plus: Table extraction and visual debugging.\n",
    "\n",
    "- [pypdf](https://pypdf.readthedocs.io/en/latest/) - Capable of splitting, merging, cropping, and transforming the pages of PDF files.\n",
    "\n",
    "- [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/) - A high-performance Python library for data extraction, analysis, conversion & manipulation of PDF (and other) documents.\n",
    "\n",
    "- [Camelot](https://camelot-py.readthedocs.io/en/master/) - This library is specifically for extracting data from tables in PDFs. This repo also has a [nice comparison](https://github.com/camelot-dev/camelot/wiki/Comparison-with-other-PDF-Table-Extraction-libraries-and-tools) of other table extraction libraries.\n",
    "\n",
    "- [LLMSherpa](https://github.com/nlmatics/llmsherpa) - The main class here is the `LayoutPDFReader`, and a good read about the problem and proposed solution is [here](https://ambikasukla.substack.com/p/efficient-rag-with-document-layout)\n",
    "\n",
    "- [unstructured](https://github.com/Unstructured-IO/unstructured) - This has components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more. \n",
    "\n",
    "- [Table Transformer](https://github.com/microsoft/table-transformer) - A deep learning model for extracting tables from unstructured documents (PDFs and images)\n",
    "\n",
    "- [Layout Parser](https://github.com/Layout-Parser/layout-parser) - This is a unified toolkit for deep learning based document image analysis which has a rich repository of deep learning models for layout detection, as well as a set of unified APIs for using them.\n",
    "\n",
    " - [marker](https://github.com/VikParuchuri/marker) - Converts PDF to markdown quickly with high accuracy.\n",
    "\n",
    " - [surya](https://github.com/VikParuchuri/surya) -  A document OCR toolkit for accurate OCR in 90+ languages, line-level text detection in any language, layout analysis (table, image, header, etc detection) in any language.\n",
    "\n",
    "### Academic PDFs\n",
    "\n",
    "- [nougat](https://github.com/facebookresearch/nougat) - This is an academic document PDF parser that understands LaTeX math and tables.\n",
    "\n",
    "- [GROBID](https://grobid.readthedocs.io/en/latest/Introduction/) - This is a a machine learning library for extracting, parsing and re-structuring raw documents such as PDF into structured XML/TEI encoded documents with a particular focus on technical and scientific publications.\n",
    "\n",
    "- [LaTeX-OCR](https://github.com/lukas-blecher/LaTeX-OCR/) - Uses a vision transformer (ViT) to convert images of equations into LaTeX code.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
